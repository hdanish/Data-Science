{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll work with text data from newsgroup postings on a variety of topics. You'll train classifiers to distinguish between the topics based on the text of the posts. Whereas with digit classification, the input is relatively dense: a 28x28 matrix of pixels, many of which are non-zero, here we'll represent each document with a \"bag-of-words\" model. As you'll see, this makes the feature representation quite sparse -- only a few words of the total vocabulary are active in any given document. The bag-of-words assumption here is that the label depends only on the words; their order is not important.\n",
    "\n",
    "The SK-learn documentation on feature extraction will prove useful:\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, stripping out metadata so that we learn classifiers that only use textual features. By default, newsgroups data is split into train and test sets. We further split the test so we have a dev set. Note that we specify 4 categories to use for this project. If you remove the categories argument from the fetch function, you'll get all 20 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (11314,)\n",
      "test label shape: (3766,)\n",
      "dev label shape: (3766,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[num_test/2:], newsgroups_test.target[num_test/2:]\n",
    "dev_data, dev_labels = newsgroups_test.data[:num_test/2], newsgroups_test.target[:num_test/2]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'test label shape:', test_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) For each of the first 5 training examples, print the text of the message along with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 :\n",
      "Text: From: cubbie@garnet.berkeley.edu (                               )\n",
      "Subject: Re: Cubs behind Marlins? How?\n",
      "Article-I.D.: agate.1pt592$f9a\n",
      "Organization: University of California, Berkeley\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: garnet.berkeley.edu\n",
      "\n",
      "\n",
      "gajarsky@pilot.njin.net writes:\n",
      "\n",
      "morgan and guzman will have era's 1 run higher than last year, and\n",
      " the cubs will be idiots and not pitch harkey as much as hibbard.\n",
      " castillo won't be good (i think he's a stud pitcher)\n",
      "\n",
      "       This season so far, Morgan and Guzman helped to lead the Cubs\n",
      "       at top in ERA, even better than THE rotation at Atlanta.\n",
      "       Cubs ERA at 0.056 while Braves at 0.059. We know it is early\n",
      "       in the season, we Cubs fans have learned how to enjoy the\n",
      "       short triumph while it is still there.\n",
      "\n",
      "\n",
      "Label: 9 - rec.sport.baseball\n",
      "\n",
      "================================================\n",
      "\n",
      "Example 2 :\n",
      "Text: From: gnelson@pion.rutgers.edu (Gregory Nelson)\n",
      "Subject: Thanks Apple: Free Ethernet on my C610!\n",
      "Article-I.D.: pion.Apr.6.12.05.34.1993.11732\n",
      "Organization: Rutgers Univ., New Brunswick, N.J.\n",
      "Lines: 26\n",
      "\n",
      "\n",
      "\tWell, I just got my Centris 610 yesterday.  It took just over two \n",
      "weeks from placing the order.  The dealer (Rutgers computer store) \n",
      "appologized because Apple made a substitution on my order.  I ordered\n",
      "the one without ethernet, but they substituted one _with_ ethernet.\n",
      "He wanted to know if that would be \"alright with me\"!!!  They must\n",
      "be backlogged on Centri w/out ethernet so they're just shipping them\n",
      "with!  \n",
      "\n",
      "\tAnyway, I'm very happy with the 610 with a few exceptions.  \n",
      "Being nosy, I decided to open it up _before_ powering it on for the first\n",
      "time.  The SCSI cable to the hard drive was only partially connected\n",
      "(must have come loose in shipping).  No big deal, but I would have been\n",
      "pissed if I tried to boot it and it wouldn't come up!\n",
      "\tThe hard drive also has an annoying high pitched whine.  I've\n",
      "heard apple will exchange it if you complain, so I might try to get\n",
      "it swapped.\n",
      "\tI am also dissappionted by the lack of soft power-on/off.  This\n",
      "wasn't mentioned in any of the literature I saw.  Also, the location\n",
      "of the reset/interupt buttons is awful.  Having keyboard control for\n",
      "these functions was much more convenient.\n",
      "\tOh, and the screen seems tojump in a wierd way on power-up.\n",
      "I've seen this mentioned by others, so it must be a...feature...\n",
      "\tAnyway, above all, it's fast.  A great machine at a great price!\n",
      "\n",
      "gnelson@physics.rutgers.edu\n",
      "\n",
      "\n",
      "Label: 4 - comp.sys.mac.hardware\n",
      "\n",
      "================================================\n",
      "\n",
      "Example 3 :\n",
      "Text: From: crypt-comments@math.ncsu.edu\n",
      "Subject: Cryptography FAQ 10/10 - References\n",
      "Organization: The Crypt Cabal\n",
      "Lines: 333\n",
      "Expires: 22 May 1993 04:00:07 GMT\n",
      "Reply-To: crypt-comments@math.ncsu.edu\n",
      "NNTP-Posting-Host: pad-thai.aktis.com\n",
      "Summary: Part 10 of 10 of the sci.crypt FAQ, References.\n",
      " History and classical methods. Modern methods. Survey and reference\n",
      " articles. Journals and conference proceedings. Electronic sources\n",
      " (FTP sites).  Related newsgroups.\n",
      "X-Last-Updated: 1993/04/16\n",
      "\n",
      "Archive-name: cryptography-faq/part10\n",
      "Last-modified: 1993/4/15\n",
      "\n",
      "\n",
      "FAQ for sci.crypt, part 10: References\n",
      "\n",
      "This is the tenth of ten parts of the sci.crypt FAQ. The parts are\n",
      "mostly independent, but you should read the first part before the rest.\n",
      "We don't have the time to send out missing parts by mail, so don't ask.\n",
      "Notes such as ``[KAH67]'' refer to the reference list in this part.\n",
      "\n",
      "The sections of this FAQ are available via anonymous FTP to rtfm.mit.edu \n",
      "as /pub/usenet/news.answers/cryptography-faq/part[xx].  The Cryptography \n",
      "FAQ is posted to the newsgroups sci.crypt, sci.answers, and news.answers \n",
      "every 21 days.\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "* Books on history and classical methods\n",
      "* Books on modern methods\n",
      "* Survey articles\n",
      "* Reference articles\n",
      "* Journals, conference proceedings\n",
      "* Other\n",
      "* Electronic sources\n",
      "* RFCs (available from [FTPRF])\n",
      "* Related newsgroups\n",
      "\n",
      "\n",
      "* Books on history and classical methods\n",
      "\n",
      "  [CF]    Lambros D. Callimahos, William F. Friedman, Military Cryptanalytics.\n",
      "\t  Aegean Park Press, ?.\n",
      "  [DEA85] Cipher A. Deavours & Louis Kruh, Machine Cryptography and\n",
      "          Modern Cryptanalysis. Artech House, 610 Washington St.,\n",
      "          Dedham, MA 02026, 1985.\n",
      "  [FRIE2] William F. Friedman, Solving German Codes in World War I.\n",
      "          Aegean Park Press, ?.\n",
      "  [GAI44] H. Gaines, Cryptanalysis, a study of ciphers and their\n",
      "          solution. Dover Publications, 1944.\n",
      "  [HIN00] F.H.Hinsley, et al., British Intelligence in the Second\n",
      "          World War. Cambridge University Press. (vol's 1, 2, 3a, 3b\n",
      "          & 4, so far).  XXX Years and authors, fix XXX\n",
      "  [HOD83] Andrew Hodges, Alan Turing: The Enigma. Burnett Books\n",
      "          Ltd., 1983\n",
      "  [KAH91] David Kahn, Seizing the Enigma. Houghton Mifflin, 1991.\n",
      "  [KAH67] D. Kahn, The Codebreakers. Macmillan Publishing, 1967.\n",
      "          [history] [The abridged paperback edition left out most\n",
      "          technical details; the original hardcover edition is\n",
      "          recommended.]\n",
      "  [KOZ84] W. Kozaczuk, Enigma. University Publications of America, 1984\n",
      "  [KUL76] S. Kullback, Statistical Methods in Cryptanalysis. Aegean\n",
      "          Park Press, 1976.\n",
      "  [SIN66] A. Sinkov, Elementary Cryptanalysis. Math. Assoc. Am. 1966.\n",
      "  [WEL82] Gordon Welchman, The Hut Six Story. McGraw-Hill, 1982.\n",
      "  [YARDL] Herbert O. Yardley, The American Black Chamber. Aegean Park\n",
      "          Press, ?.\n",
      "\n",
      "* Books on modern methods\n",
      "\n",
      "  [BEK82] H. Beker, F. Piper, Cipher Systems. Wiley, 1982.\n",
      "  [BRA88] G. Brassard, Modern Cryptology: a tutorial.\n",
      "          Spinger-Verlag, 1988.\n",
      "  [DEN82] D. Denning, Cryptography and Data Security. Addison-Wesley\n",
      "          Publishing Company, 1982.\n",
      "  [KOB89] N. Koblitz, A course in number theory and cryptography.\n",
      "          Springer-Verlag, 1987.\n",
      "  [KON81] A. Konheim, Cryptography: a primer. Wiley, 1981.\n",
      "  [MEY82] C. Meyer and S. Matyas, Cryptography: A new dimension in\n",
      "          computer security. Wiley, 1982.\n",
      "  [PAT87] Wayne Patterson, Mathematical Cryptology for Computer\n",
      "          Scientists and Mathematicians. Rowman & Littlefield, 1987.\n",
      "  [PFL89] C. Pfleeger, Security in Computing. Prentice-Hall, 1989.\n",
      "  [PRI84] W. Price, D. Davies, Security for computer networks. Wiley, 1984. \n",
      "  [RUE86] R. Rueppel, Design and Analysis of Stream Ciphers.\n",
      "          Springer-Verlag, 1986.\n",
      "  [SAL90] A. Saloma, Public-key cryptography. Springer-Verlag, 1990.\n",
      "  [WEL88] D. Welsh, Codes and Cryptography. Claredon Press, 1988.\n",
      "\n",
      "* Survey articles\n",
      "\n",
      "  [ANG83] D. Angluin, D. Lichtenstein, Provable Security in Crypto-\n",
      "          systems: a survey. Yale University, Department of Computer\n",
      "          Science, #288, 1983.\n",
      "  [BET90] T. Beth, Algorithm engineering for public key algorithms.\n",
      "          IEEE Selected Areas of Communication, 1(4), 458--466,\n",
      "          1990.\n",
      "  [DAV83] M. Davio, J. Goethals, Elements of cryptology. in Secure\n",
      "          Digital Communications, G. Longo ed., 1--57, 1983.\n",
      "  [DIF79] W. Diffie, M. Hellman, Privacy and Authentication: An\n",
      "          introduction to cryptography. IEEE proceedings, 67(3),\n",
      "          397--427, 1979.\n",
      "  [DIF88] W. Diffie, The first ten years of public key cryptography.\n",
      "          IEEE proceedings, 76(5), 560--577, 1988.\n",
      "  [FEI73] H. Feistel, Cryptography and Computer Privacy. Scientific \n",
      "          American, 228(5), 15--23, 1973.\n",
      "  [FEI75] H. Feistel, H, W. Notz, J. Lynn Smith. Some cryptographic\n",
      "          techniques for machine-to-machine data communications,\n",
      "          IEEE IEEE proceedings, 63(11), 1545--1554, 1975.\n",
      "  [HEL79] M. Hellman, The mathematics of public key cryptography.\n",
      "          Scientific American, 130--139, 1979.\n",
      "  [LAK83] S. Lakshmivarahan, Algorithms for public key\n",
      "          cryptosystems. In Advances in Computers, M. Yovtis ed.,\n",
      "          22, Academic Press, 45--108, 1983.\n",
      "  [LEM79] A. Lempel, Cryptology in transition, Computing Surveys,\n",
      "          11(4), 285--304, 1979.\n",
      "  [MAS88] J. Massey, An introduction to contemporary cryptology, IEEE\n",
      "          proceedings, 76(5), 533--549, 1988.\n",
      "  [SIM91] G. Simmons (ed.), Contemporary Cryptology: the Science of\n",
      "          Information Integrity. IEEE press, 1991.\n",
      "\n",
      "* Reference articles\n",
      "\n",
      "  [AND83] D. Andelman, J. Reeds, On the cryptanalysis of rotor and\n",
      "          substitution-permutation networks. IEEE Trans. on Inform.\n",
      "          Theory, 28(4), 578--584, 1982.\n",
      "  [BEN87] John Bennett, Analysis of the Encryption Algorithm Used in\n",
      "          the WordPerfect Word Processing Program. Cryptologia 11(4),\n",
      "          206--210, 1987.\n",
      "  [BER91] H. A. Bergen and W. J. Caelli, File Security in WordPerfect\n",
      "          5.0. Cryptologia 15(1), 57--66, January 1991.\n",
      "  [BIH91] E. Biham and A. Shamir, Differential cryptanalysis of\n",
      "          DES-like cryptosystems. Journal of Cryptology, vol. 4, #1,\n",
      "          3--72, 1991.\n",
      "  [BI91a] E. Biham, A. Shamir, Differential cryptanalysis of Snefru,\n",
      "          Khafre, REDOC-II, LOKI and LUCIFER. In Proceedings of CRYPTO\n",
      "          '91, ed. by J. Feigenbaum, 156--171, 1992.\n",
      "  [BOY89] J. Boyar, Inferring Sequences Produced by Pseudo-Random\n",
      "          Number Generators. Journal of the ACM, 1989.\n",
      "  [BRI86] E. Brickell, J. Moore, M. Purtill, Structure in the\n",
      "          S-boxes of DES. In Proceedings of CRYPTO '86, A. M. Odlyzko\n",
      "          ed., 3--8, 1987.\n",
      "  [BRO89] L. Brown, A proposed design for an extended DES, Computer\n",
      "          Security in the Computer Age. Elsevier Science Publishers\n",
      "          B.V. (North Holland), IFIP, W. J. Caelli ed., 9--22, 1989.\n",
      "  [BRO90] L. Brown, J. Pieprzyk, J. Seberry, LOKI - a cryptographic\n",
      "          primitive for authentication and secrecy applications.\n",
      "          In Proceedings of AUSTCRYPT 90, 229--236, 1990.\n",
      "  [CAE90] H. Gustafson, E. Dawson, W. Caelli, Comparison of block\n",
      "          ciphers. In Proceedings of AUSCRYPT '90, J. Seberry and J.\n",
      "          Piepryzk eds., 208--220, 1990.\n",
      "  [CAM93] K. W. Campbell, M. J. Wiener, Proof the DES is Not a Group.\n",
      "          In Proceedings of CRYPTO '92, 1993.\n",
      "  [ELL88] Carl M. Ellison, A Solution of the Hebern Messages. Cryptologia,\n",
      "          vol. XII, #3, 144-158, Jul 1988.\n",
      "  [EVE83] S. Even, O. Goldreich, DES-like functions can generate the\n",
      "          alternating group. IEEE Trans. on Inform. Theory, vol. 29,\n",
      "          #6, 863--865, 1983.\n",
      "  [GAR91] G. Garon, R. Outerbridge, DES watch: an examination of the\n",
      "          sufficiency of the Data Encryption Standard for financial\n",
      "          institutions in the 1990's. Cryptologia, vol. XV, #3,\n",
      "          177--193, 1991.\n",
      "  [GIL80] Gillogly, ?. Cryptologia 4(2), 1980.\n",
      "  [GM82]  Shafi Goldwasser, Silvio Micali, Probabilistic Encryption and\n",
      "\t  How To Play Mental Poker Keeping Secret All Partial Information.\n",
      "\t  Proceedings of the Fourteenth Annual ACM Symposium on Theory of\n",
      "\t  Computing, 1982.\n",
      "  [HUM83] D. G. N. Hunter and A. R. McKenzie, Experiments with\n",
      "          Relaxation Algorithms for Breaking Simple Substitution\n",
      "          Ciphers. Computer Journal 26(1), 1983.\n",
      "  [KAM78] J. Kam, G. Davida, A structured design of substitution-\n",
      "          permutation encryption networks. IEEE Trans. Information\n",
      "          Theory, 28(10), 747--753, 1978.\n",
      "  [KIN78] P. Kinnucan, Data encryption gurus: Tuchman and Meyer.\n",
      "          Cryptologia, vol. II #4, 371--XXX, 1978.\n",
      "  [KRU88] Kruh, ?. Cryptologia 12(4), 1988.\n",
      "  [LAI90] X. Lai, J. Massey, A proposal for a new block encryption \n",
      "          standard. EUROCRYPT 90, 389--404, 1990.\n",
      "  [LUB88] C. Rackoff, M. Luby, How to construct psuedorandom\n",
      "          permutations from psuedorandom functions. SIAM Journal of\n",
      "          Computing, vol. 17, #2, 373--386, 1988.\n",
      "  [MAS88] J. Massey, An introduction to contemporary cryptology.\n",
      "          IEEE proceedings, 76(5), 533--549, 1988.\n",
      "  [ME91a] R. Merkle, Fast software encryption functions. In Proceedings\n",
      "          of CRYPTO '90, Menezes and Vanstone ed., 476--501, 1991.\n",
      "  [MEY78] C. Meyer, Ciphertext/plaintext and ciphertext/key\n",
      "          dependence vs. number of rounds for the Data Encryption\n",
      "          Standard. AFIPS Conference proceedings, 47, 1119--1126,\n",
      "          1978.\n",
      "  [NBS77] Data Encryption Standard. National Bureau of Standards,\n",
      "          FIPS PUB 46, Washington, DC, January 1977.\n",
      "  [REE77] J. Reeds, `Cracking' a Random Number Generator.\n",
      "          Cryptologia 1(1), 20--26, 1977.\n",
      "  [REE84] J. A. Reeds and P. J. Weinberger, File Security and the UNIX\n",
      "          Crypt Command. AT&T Bell Laboratories Technical Journal,\n",
      "          Vol. 63 #8, part 2, 1673--1684, October, 1984.\n",
      "  [SHA49] C. Shannon, Communication Theory of Secrecy Systems. Bell\n",
      "          System Technical Journal 28(4), 656--715, 1949.\n",
      "  [SHE88] B. Kaliski, R. Rivest, A. Sherman, Is the Data Encryption\n",
      "          Standard a Group. Journal of Cryptology, vol. 1, #1,\n",
      "          1--36, 1988.\n",
      "  [SHI88] A. Shimizu, S. Miyaguchi, Fast data encipherment algorithm\n",
      "          FEAL. EUROCRYPT '87, 267--278, 1988.\n",
      "  [SOR84] A. Sorkin, LUCIFER: a cryptographic algorithm.\n",
      "          Cryptologia, 8(1), 22--35, 1984.\n",
      "\t\n",
      "* Journals, conference proceedings\n",
      "\n",
      "  CRYPTO\n",
      "  Eurocrypt\n",
      "  IEEE Transactions on Information Theory\n",
      "  Cryptologia: a cryptology journal, quarterly since Jan 1977.\n",
      "          Cryptologia; Rose-Hulman Institute of Technology; Terre Haute\n",
      "          Indiana 47803 [general: systems, analysis, history, ...]\n",
      "  Journal of Cryptology; International Association for Cryptologic\n",
      "          Research; published by Springer Verlag (quarterly since\n",
      "          1988).\n",
      "  The Cryptogram (Journal of the American Cryptogram Association);\n",
      "          18789 West Hickory Street; Mundelein, IL 60060; [primarily\n",
      "          puzzle cryptograms of various sorts]\n",
      "  Cryptosystems Journal, Published by Tony Patti, P.O. Box 188,\n",
      "          Newtown PA, USA 18940-0188 or tony_s_patti@cup.portal.com.\n",
      "\t  Publisher's comment: Includes complete cryptosystems with\n",
      "\t  source and executable programs on diskettes. Tutorial. The\n",
      "\t  typical cryptosystems supports multi-megabit keys and Galois\n",
      "\t  Field arithmetic. Inexpensive hardware random number\n",
      "\t  generator details.\n",
      "\n",
      "  Computer and Communication Security Reviews, published by Ross Anderson.\n",
      "\t  Sample issue available from various ftp sites, including\n",
      "\t  black.ox.ac.uk.  Editorial c/o rja14@cl.cam.ac.uk.  Publisher's\n",
      "\t  comment: We review all the conference proceedings in this field,\n",
      "\t  including not just Crypto and Eurocrypt, but regional gatherings\n",
      "\t  like Auscrypt and Chinacrypt. We also abstract over 50 journals,\n",
      "\t  and cover computer security as well as cryptology, so readers can\n",
      "\t  see the research trends in applications as well as theory.\n",
      "\n",
      "* Other\n",
      "\n",
      "  Address of note: Aegean Park Press, P.O. Box 2837, Laguna Hills, CA\n",
      "  92654-0837. Answering machine at 714-586-8811.\n",
      "\n",
      "  The ``Orange Book'' is DOD 5200.28-STD, published December 1985 as\n",
      "  part of the ``rainbow book'' series. Write to Department of Defense,\n",
      "  National Security Agency, ATTN: S332, 9800 Savage Road, Fort Meade, MD\n",
      "  20755-6000, and ask for the Trusted Computer System Evaluation\n",
      "  Criteria. Or call 301-766-8729.\n",
      "\n",
      "  [BAMFD] Bamford, The Puzzle Palace. Penguin Books, ?.\n",
      "  [GOO83] I. J. Good, Good Thinking: the foundations of probability and\n",
      "          its applications. University of Minnesota Press, 1983.\n",
      "  [KNU81] D. E. Knuth, The Art of Computer Programming, volume 2:\n",
      "          Seminumerical Algorithms. Addison-Wesley, 1981.\n",
      "  [KUL68] Soloman Kullbach, Information Theory and Statistics.\n",
      "          Dover, 1968.\n",
      "  [YAO88] A. Yao, Computational Information Theory. In Complexity in\n",
      "          Information Theory, ed. by Abu-Mostafa, 1988.\n",
      "\n",
      "* How may one obtain copies of FIPS and ANSI standards cited herein?\n",
      "\n",
      "  Many textbooks on cryptography contain complete reprints of the FIPS\n",
      "  standards, which are not copyrighted.\n",
      "\n",
      "  The following standards may be ordered from the\n",
      "      U.S. Department of Commerce, National Technical Information Service,\n",
      "      Springfield, VA 22161.\n",
      "\n",
      "      FIPS PUB 46-1 \"Data Encryption Standard\"  (this is DES)\n",
      "      FIPS PUB 74   \"Guidelines for Implementing as Using the NBS DES\"\n",
      "      FIPS PUB 81   \"DES Modes of Operation\"\n",
      "      FIPS PUB 113  \"Computer Data Authentication\" (using DES)\n",
      "\n",
      "  The following standards may be ordered from the\n",
      "      American National Standards Institute Sales Office,\n",
      "      1430 Broadway, New York, NY 10018.\n",
      "      Phone 212.642.4900\n",
      "\n",
      "      ANSI X3.92-1981  \"Data Encryption Algorithm\" (identical to FIPS 46-1)\n",
      "      ANSI X3.106-1983 \"DEA Modes of Operation\"    (identical to FIPS 113)\n",
      "\n",
      "  Notes:  Figure 3 in FIPS PUB 46-1 is in error, but figure 3 in X3.92-1981\n",
      "      is correct.  The text is correct in both publications.\n",
      "\n",
      "\n",
      "* Electronic sources\n",
      "\n",
      "  Anonymous ftp:\n",
      "\n",
      "  [FTPBK] ftp.uu.net:bsd-sources/usr.bin/des/\n",
      "  [FTPCB] ftp.uu.net:usenet/comp.sources.unix/volume10/cbw/\n",
      "  [FTPDF] ftp.funet.fi:pub/unix/security/destoo.tar.Z\n",
      "  [FTPEY] ftp.psy.uq.oz.au:pub/DES/\n",
      "  [FTPMD] rsa.com:?\n",
      "  [FTPMR] cl-next3.cl.msu.edu:pub/crypt/newdes.tar.Z\n",
      "  [FTPOB] ftp.3com.com:Orange-book\n",
      "  [FTPPF] prep.ai.mit.edu:pub/lpf/\n",
      "  [FTPPK] ucsd.edu:hamradio/packet/tcpip/crypto/des.tar.Z\n",
      "  [FTPRF] nic.merit.edu:documents/rfc/\n",
      "  [FTPSF] beta.xerox.com:pub/hash/\n",
      "  [FTPSO] chalmers.se:pub/des/des.1.0.tar.Z\n",
      "  [FTPUF] ftp.uu.net:usenet/comp.sources.unix/volume28/ufc-crypt/\n",
      "  [FTPWP] garbo.uwasa.fi:pc/util/wppass2.zip\n",
      "\n",
      "* RFCs (available from [FTPRF])\n",
      "\n",
      "\n",
      "1424  Kaliski, B.  Privacy Enhancement for Internet Electronic Mail: Part IV:\n",
      "      Key Certification and Related Services.  1993 February; 9 p. (Format:\n",
      "      TXT=17538 bytes)\n",
      "\n",
      "1423  Balenson, D.  Privacy Enhancement for Internet Electronic Mail: Part\n",
      "      III: Algorithms, Modes, and Identifiers.  1993 February; 14 p. (Format:\n",
      "      TXT=33278 bytes)  (Obsoletes RFC 1115)\n",
      "\n",
      "1422  Kent, S.  Privacy Enhancement for Internet Electronic Mail: Part II:\n",
      "      Certificate-Based Key Management.  1993 February; 32 p. (Format:\n",
      "      TXT=86086 bytes)  (Obsoletes RFC 1114)\n",
      "\n",
      "1421  Linn, J.  Privacy Enhancement for Internet Electronic Mail: Part I:\n",
      "      Message Encryption and Authentication Procedures.  1993 February; 42 p.\n",
      "      (Format: TXT=103895 bytes)  (Obsoletes RFC 1113)\n",
      "\n",
      "\n",
      "* Related newsgroups\n",
      "\n",
      "  There are other newsgroups which a sci.crypt reader might want also to\n",
      "  read.  Some have their own FAQ as well.\n",
      "\n",
      "  alt.comp.compression\t\tdiscussion of compression algorithms and code\n",
      "  alt.security\t\t\tgeneral security discussions\n",
      "  alt.security.index\t\tindex to alt.security\n",
      "  alt.security.pgp\t\tdiscussion of PGP\n",
      "  alt.security.ripem\t\tdiscussion of RIPEM\n",
      "  alt.society.civil-liberty\tgeneral civil liberties, including privacy\n",
      "  comp.org.eff.news\t\tNews reports from EFF\n",
      "  comp.org.eff.talk\t\tdiscussion of EFF related issues\n",
      "  comp.patents\t\t\tdiscussion of S/W patents, including RSA\n",
      "  comp.risks\t\t\tsome mention of crypto and wiretapping\n",
      "  comp.society.privacy\t\tgeneral privacy issues\n",
      "  comp.security.announce\tannouncements of security holes\n",
      "  misc.legal.computing\t\t\n",
      "  sci.math\t\t\tgeneral math discussion\n",
      "\n",
      "\n",
      "Label: 11 - sci.crypt\n",
      "\n",
      "================================================\n",
      "\n",
      "Example 4 :\n",
      "Text: From:  ()\n",
      "Subject: Re: Quadra SCSI Problems???\n",
      "Organization: Apple Computer Inc.\n",
      "Lines: 28\n",
      "\n",
      "> ATTENTION: Mac Quadra owners: Many storage industry experts have\n",
      "> concluded that Mac Quadras suffer from timing irregularities deviating\n",
      "> from the standard SCSI specification. This results in silent corruption\n",
      "> of data when used with some devices, including ultra-modern devices.\n",
      "> Although I will not name the devices, since it is not their fault, an\n",
      "> example would be a Sony 3.5 inch MO, without the special \"Mac-compatible\"\n",
      "> firmware installed. One solution, sometimes, is to disable \"blind writes\"\n",
      "> \n",
      "To the best of my knowledge there aren't any problems with Quadras and\n",
      "blind transfers.  Trouble with blind transfers usually means the programmer\n",
      "screwed up the TIBs or didn't test their driver with the device in question.\n",
      "Well designed TIBs poll or loop at every point where delays of >16Âµsec occur.\n",
      "This usually occurs at the first byte of each block of a transfer but some\n",
      "devices can \"hiccup\" in the middle of blocks.  If this happens in the middle\n",
      "of a blind transfer there is the possibility of losing or gaining a byte\n",
      "depending on which direction the tranfer was going.  In anycase the SCSI Manager\n",
      "will eventually return a phase error at the end of the transaction because\n",
      "it is out of sync.  Actual data loss would only occur if the driver didn't\n",
      "pay attention to the errors coming back.\n",
      "\n",
      "Note that this effect is not caused by anything actually on the SCSI Bus but\n",
      "rather by the transfer loops inside the SCSI Manager.  The problem occurs when\n",
      "the processor bus errors trying to access the SCSI chip when the next byte\n",
      "hasn't been clocked yet.  Also note that the Bus Error is dealt with by a bus\n",
      "error handler and doesn't crash the machine...\n",
      "\n",
      "Clinton Bauder\n",
      "Apple Computer\n",
      "\n",
      "\n",
      "Label: 4 - comp.sys.mac.hardware\n",
      "\n",
      "================================================\n",
      "\n",
      "Example 5 :\n",
      "Text: From: keith@cco.caltech.edu (Keith Allan Schneider)\n",
      "Subject: Re: <Political Atheists?\n",
      "Organization: California Institute of Technology, Pasadena\n",
      "Lines: 14\n",
      "NNTP-Posting-Host: lloyd.caltech.edu\n",
      "\n",
      "bobbe@vice.ICO.TEK.COM (Robert Beauchaine) writes:\n",
      "\n",
      ">To show that the examples I and others\n",
      ">have provided are *not* counter examples of your supposed inherent\n",
      ">moral hypothesis, you have to successfully argue that\n",
      ">domestication removes or alters this morality.\n",
      "\n",
      "I think that domestication will change behavior to a large degree.\n",
      "Domesticated animals exhibit behaviors not found in the wild.  I\n",
      "don't think that they can be viewed as good representatives of the\n",
      "wild animal kingdom, since they have been bred for thousands of years\n",
      "to produce certain behaviors, etc.\n",
      "\n",
      "keith\n",
      "\n",
      "\n",
      "Label: 0 - alt.atheism\n",
      "\n",
      "================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def P1(num_examples=5):\n",
    "### STUDENT START ###\n",
    "    for i in range(num_examples):\n",
    "        print \"Example %i :\\nText: %s\\n\\nLabel: %s - %s\" % (i+1, train_data[i], train_labels[i], newsgroups_train.target_names[train_labels[i]])\n",
    "        print \"\\n================================================\\n\"\n",
    "### STUDENT END ###\n",
    "P1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Use CountVectorizer to turn the raw training text into feature vectors. You should use the fit_transform function, which makes 2 passes through the data: first it computes the vocabulary (\"fit\"), second it converts the raw text into feature vectors using the vocabulary (\"transform\").\n",
    "\n",
    "The vectorizer has a lot of options. To get familiar with some of them, write code to answer these questions:\n",
    "\n",
    "a. The output of the transform (also of fit_transform) is a sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html. What is the size of the vocabulary? What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero? Hint: use \"nnz\" and \"shape\" attributes.\n",
    "\n",
    "b. What are the 0th and last feature strings (in alphabetical order)? Hint: use the vectorizer's get_feature_names function.\n",
    "\n",
    "c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. Confirm the training vectors are appropriately shaped. Now what's the average number of non-zero features per example?\n",
    "\n",
    "d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram character features. What size vocabulary does this yield?\n",
    "\n",
    "e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. What size vocabulary does this yield?\n",
    "\n",
    "f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? Hint: build a vocabulary for both train and dev and look at the size of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part a\n",
      "Size of the vocabulary is: 26879\n",
      "Average number of non-zero features per example is: 96.705998\n",
      "Fraction of entries in the matrix that are non-zero: 0.003598\n",
      "\n",
      "Part b\n",
      "0th feature is: 00\n",
      "Last feature is zyxel\n",
      "\n",
      "Part c\n",
      "New vocabulary size is: 4\n",
      "Average number of non-zero features per example is: 0.268437\n",
      "\n",
      "Part d\n",
      "Size of the vocabulary is: 510583\n",
      "\n",
      "Part e\n",
      "Size of the vocabulary is: 3064\n",
      "\n",
      "Part f\n",
      "Fraction of words in the dev data missing from the vocabulary is: 0.247876\n"
     ]
    }
   ],
   "source": [
    "def P2():\n",
    "### STUDENT START ###\n",
    "    cv = CountVectorizer()\n",
    "    V = cv.fit_transform(train_data)\n",
    "    \n",
    "    # Part a\n",
    "    print \"Part a\"\n",
    "    print \"Size of the vocabulary is: %s\" % V.shape[1]\n",
    "    avg = (float(V.nnz) / float(V.shape[0]))\n",
    "    print \"Average number of non-zero features per example is: %f\" % avg\n",
    "    fraction = (float(V.nnz) / float(V.shape[0] * V.shape[1]))\n",
    "    print \"Fraction of entries in the matrix that are non-zero: %f\\n\" % fraction\n",
    "    \n",
    "    # Part b\n",
    "    print \"Part b\"\n",
    "    features = cv.get_feature_names()\n",
    "    # Assume that alphabetical sorting also includes numeric characters\n",
    "    sorted(features)\n",
    "    print \"0th feature is: %s\" % features[0]\n",
    "    print \"Last feature is %s\\n\" % features[len(features)-1]\n",
    "    \n",
    "    # Part c\n",
    "    print \"Part c\"\n",
    "    vocab = [\"atheism\", \"graphics\", \"space\", \"religion\"]\n",
    "    cv2 = CountVectorizer(vocabulary=vocab)\n",
    "    V2 = cv2.fit_transform(train_data)\n",
    "    print \"New vocabulary size is: %s\" % V2.shape[1]\n",
    "    avg2 = (float(V2.nnz) / float(V2.shape[0]))\n",
    "    print \"Average number of non-zero features per example is: %f\\n\" % avg2\n",
    "    \n",
    "    # Part d\n",
    "    print \"Part d\"\n",
    "    cv3 = CountVectorizer(analyzer='word', ngram_range=(2,3))\n",
    "    V3 = cv3.fit_transform(train_data)\n",
    "    print \"Size of the vocabulary is: %s\\n\" % V3.shape[1]\n",
    "    \n",
    "    # Part e\n",
    "    print \"Part e\"\n",
    "    cv4 = CountVectorizer(min_df=10)\n",
    "    V4 = cv4.fit_transform(train_data)\n",
    "    print \"Size of the vocabulary is: %s\\n\" % V4.shape[1]\n",
    "    \n",
    "    # Part f\n",
    "    print \"Part f\"\n",
    "    cv5 = CountVectorizer()\n",
    "    V5 = cv5.fit_transform(dev_data)\n",
    "    features_dev = cv5.get_feature_names()\n",
    "    missing_dev = [w for w in features_dev if w not in features]\n",
    "    fraction_missing = (float(len(missing_dev))/float(len(features_dev)))\n",
    "    print \"Fraction of words in the dev data missing from the vocabulary is: %f\" % fraction_missing\n",
    "### STUDENT END ###\n",
    "P2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Use the default CountVectorizer options and report the f1 score (use metrics.f1_score) for a k nearest neighbors classifier; find the optimal value for k. Also fit a Multinomial Naive Bayes model and find the optimal value for alpha. Finally, fit a logistic regression model and find the optimal value for the regularization strength C using l2 regularization. A few questions:\n",
    "\n",
    "a. Why doesn't nearest neighbors work well for this problem?\n",
    "\n",
    "b. Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
    "\n",
    "c. Logistic regression estimates a weight vector for each class, which you can access with the coef\\_ attribute. Output the sum of the squared weight values for each class for each setting of the C parameter. Briefly explain the relationship between the sum and the value of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for KNN is: 0.41805566212\n",
      "Best score is: 0.441494591937 for k: 200\n",
      "\n",
      "F1 score for MNB is: 0.775166321854\n",
      "Best score is: 0.828416912488 for alpha: 0.01\n",
      "\n",
      "F1 score for logistic is: 0.708473977649\n",
      "Best score is: 0.771386430678 for C: 0.5\n",
      "\n",
      "For C=0.000100\n",
      "Class 1 : 0.007702\n",
      "Class 2 : 0.011941\n",
      "Class 3 : 0.009435\n",
      "Class 4 : 0.009103\n",
      "\n",
      "For C=0.001000\n",
      "Class 1 : 0.165093\n",
      "Class 2 : 0.200953\n",
      "Class 3 : 0.180671\n",
      "Class 4 : 0.187243\n",
      "\n",
      "For C=0.010000\n",
      "Class 1 : 2.541462\n",
      "Class 2 : 2.939709\n",
      "Class 3 : 2.862469\n",
      "Class 4 : 2.250029\n",
      "\n",
      "For C=0.100000\n",
      "Class 1 : 27.129310\n",
      "Class 2 : 24.663097\n",
      "Class 3 : 27.462182\n",
      "Class 4 : 23.024042\n",
      "\n",
      "For C=0.500000\n",
      "Class 1 : 102.605362\n",
      "Class 2 : 83.095268\n",
      "Class 3 : 99.031644\n",
      "Class 4 : 89.009789\n",
      "\n",
      "For C=1.000000\n",
      "Class 1 : 166.961609\n",
      "Class 2 : 130.937772\n",
      "Class 3 : 158.001019\n",
      "Class 4 : 145.776023\n",
      "\n",
      "For C=2.000000\n",
      "Class 1 : 257.974472\n",
      "Class 2 : 197.938700\n",
      "Class 3 : 239.938134\n",
      "Class 4 : 226.562228\n",
      "\n",
      "For C=5.000000\n",
      "Class 1 : 422.762717\n",
      "Class 2 : 322.545481\n",
      "Class 3 : 389.914550\n",
      "Class 4 : 377.803888\n",
      "\n",
      "For C=10.000000\n",
      "Class 1 : 584.583778\n",
      "Class 2 : 447.515153\n",
      "Class 3 : 538.925421\n",
      "Class 4 : 530.694130\n"
     ]
    }
   ],
   "source": [
    "def P3():\n",
    "### STUDENT START ###\n",
    "    cv = CountVectorizer()\n",
    "    v_train_data = cv.fit_transform(train_data)\n",
    "    v_dev_data = cv.transform(dev_data)\n",
    "    \n",
    "    # Set parameters for GridSearchCV\n",
    "    k_values = {'n_neighbors': [1, 3, 5, 7, 9, 15, 20, 50, 100, 200, 300]}\n",
    "    alphas = {'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "    C_values = {'C': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "\n",
    "    # Setup classifiers\n",
    "    knn = KNeighborsClassifier()\n",
    "    mnb = MultinomialNB()\n",
    "    logistic = LogisticRegression(penalty='l2')\n",
    "    \n",
    "    # Fit and predict data and also find best param\n",
    "    gs_knn = GridSearchCV(knn, k_values)\n",
    "    gs_knn.fit(v_train_data, train_labels)\n",
    "    knn_pred = gs_knn.predict(v_dev_data)\n",
    "    print \"F1 score for KNN is: %s\" % (metrics.f1_score(dev_labels, knn_pred, average=\"weighted\"))\n",
    "    print \"Best score is: %s for k: %s\\n\" % (gs_knn.best_score_, gs_knn.best_estimator_.n_neighbors)    \n",
    "    \n",
    "    gs_mnb = GridSearchCV(mnb, alphas)\n",
    "    gs_mnb.fit(v_train_data, train_labels)\n",
    "    mnb_pred = gs_mnb.predict(v_dev_data)\n",
    "    print \"F1 score for MNB is: %s\" % (metrics.f1_score(dev_labels, mnb_pred, average=\"weighted\"))\n",
    "    print \"Best score is: %s for alpha: %s\\n\" % (gs_mnb.best_score_, gs_mnb.best_estimator_.alpha)\n",
    "    \n",
    "    gs_logistic = GridSearchCV(logistic, C_values)\n",
    "    gs_logistic.fit(v_train_data, train_labels)\n",
    "    logistic_pred = gs_logistic.predict(v_dev_data)\n",
    "    print \"F1 score for logistic is: %s\" % (metrics.f1_score(dev_labels, logistic_pred, average=\"weighted\"))\n",
    "    print \"Best score is: %s for C: %s\" % (gs_logistic.best_score_, gs_logistic.best_estimator_.C)\n",
    "    \n",
    "    # Calculate sum of squared weights for different C values and print them out\n",
    "    for cval in [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]:\n",
    "        lgstc = LogisticRegression(penalty='l2', C=cval)\n",
    "        lgstc.fit(v_train_data, train_labels)\n",
    "        print \"\\nFor C=%f\" % cval\n",
    "        for i in range(len(lgstc.coef_)):\n",
    "            l = [x ** 2 for x in lgstc.coef_[i]]\n",
    "            print \"Class %i : %f\" % (i+1, sum(l))\n",
    "### STUDENT END ###\n",
    "P3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "\n",
    "a. Nearest neighbors doesn't work well for this problem because we have a set of large features where many of them will be 0. KNN can have issues looking at data more locally based on the structre and so for our data this might not be good.\n",
    "\n",
    "b. Logistic regression might not work as well as Naive Bayes because Naive Bayes makes certain assumptions on the data (independence assumption). For a multinomial Naive Bayes, this works well as it can fall under the bag of words model and the classifier can becomes linear.\n",
    "\n",
    "c. As the value of C increases, so does the value of the sum. More notably, the value of the sum increases at a greater rate for values of C less than 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Train a logistic regression model. Find the 5 features with the largest weights for each label -- 20 features in total. Create a table with 20 rows and 4 columns that shows the weight for each of these features for each of the labels. Create the table again with bigram features. Any surprising features in this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature\t\tClass1\t\tClass2\t\tClass3\t\tClass4\n",
      "deletion\t\t1.12513119345\t\t-0.39841083985\t\t-0.42022106407\t\t-0.395128526973\n",
      "atheists\t\t1.03093486493\t\t-0.0973560415053\t\t-0.319943429936\t\t-0.835292582551\n",
      "bobby\t\t0.9897471964\t\t-0.220775008629\t\t-0.340817258121\t\t-0.463395066124\n",
      "religion\t\t0.953614490073\t\t-0.616840615191\t\t-0.792681834034\t\t-0.0642231255908\n",
      "atheism\t\t0.939998283429\t\t-0.410457879501\t\t-0.449342762252\t\t-0.434152709033\n",
      "graphics\t\t-0.75823600116\t\t1.93686077286\t\t-1.33616983306\t\t-0.76299171418\n",
      "image\t\t-0.582821283298\t\t1.34659164693\t\t-0.825693260013\t\t-0.46832020599\n",
      "file\t\t-0.334726753357\t\t1.26658922474\t\t-0.806429309715\t\t-0.626804295122\n",
      "3d\t\t-0.358930701764\t\t1.12491197486\t\t-0.702368038495\t\t-0.378583775093\n",
      "computer\t\t0.144051471463\t\t0.977994840988\t\t-0.682062756972\t\t-0.487391555175\n",
      "space\t\t-1.2602580532\t\t-1.31654817318\t\t2.16223398512\t\t-1.17093038323\n",
      "orbit\t\t-0.413841883434\t\t-0.671618908181\t\t1.22493885818\t\t-0.629582250172\n",
      "nasa\t\t-0.572585412953\t\t-0.480072600958\t\t1.011301188\t\t-0.467851646587\n",
      "launch\t\t-0.470818085733\t\t-0.465373951586\t\t0.936407416465\t\t-0.332317274149\n",
      "spacecraft\t\t-0.355374355744\t\t-0.393485180793\t\t0.920112548183\t\t-0.380686584541\n",
      "christians\t\t-0.740051734388\t\t-0.409400793719\t\t-0.525226424378\t\t1.14805953513\n",
      "christian\t\t-0.607717476595\t\t-0.418689512709\t\t-0.270118030078\t\t1.11788781437\n",
      "blood\t\t-0.533102659614\t\t-0.106651370597\t\t-0.316265529743\t\t1.05472152867\n",
      "fbi\t\t-0.308900330342\t\t-0.273616377746\t\t-0.448101618561\t\t0.912679487341\n",
      "order\t\t-0.793520263076\t\t-0.0793257133207\t\t-0.149140809962\t\t0.905447474355\n",
      "\n",
      "\n",
      "\n",
      "Feature\t\tClass1\t\tClass2\t\tClass3\t\tClass4\n",
      "claim that\t\t0.771698071569\t\t-0.257653251983\t\t-0.352097959986\t\t-0.200633418496\n",
      "was just\t\t0.677626900059\t\t-0.192912511316\t\t-0.19795115826\t\t-0.301989445879\n",
      "cheers kent\t\t0.649149524313\t\t-0.882279391181\t\t-0.821938595645\t\t0.601524839428\n",
      "look up\t\t0.633789294667\t\t-0.238280722174\t\t-0.195127709747\t\t-0.170538423279\n",
      "you are\t\t0.569419319422\t\t-0.31837225726\t\t-0.577109301441\t\t0.000968958001937\n",
      "looking for\t\t-0.75560959898\t\t1.31962849748\t\t-0.613629741449\t\t-0.699902638992\n",
      "comp graphics\t\t-0.379820248448\t\t1.03725806791\t\t-0.470808758902\t\t-0.396651192397\n",
      "in advance\t\t-0.544984984712\t\t0.972296488655\t\t-0.531001270225\t\t-0.507334732856\n",
      "is there\t\t-0.428448084078\t\t0.912606628457\t\t-0.56871989065\t\t-0.341539992275\n",
      "out there\t\t-0.32424483544\t\t0.896315529695\t\t-0.577895155581\t\t-0.332797374174\n",
      "the space\t\t-0.314010856109\t\t-0.645740197467\t\t1.03012481002\t\t-0.324541559344\n",
      "the moon\t\t-0.404880076642\t\t-0.576409925754\t\t0.951738232908\t\t-0.240659586704\n",
      "sci space\t\t-0.317696343442\t\t-0.388480407749\t\t0.738843615473\t\t-0.27485529382\n",
      "and such\t\t-0.242352362023\t\t-0.399630688192\t\t0.688136171498\t\t-0.256962896066\n",
      "sherzer methodology\t\t-0.157719201641\t\t-0.226508748013\t\t0.675556378115\t\t-0.163758571245\n",
      "ignorance is\t\t-0.227878121764\t\t-0.253135416803\t\t-0.211356554559\t\t0.745675613216\n",
      "such lunacy\t\t-0.149082302671\t\t-0.216340618166\t\t-0.190670242412\t\t0.706568429096\n",
      "compuserve com\t\t-0.132642240024\t\t-0.211822317108\t\t-0.200881359729\t\t0.701468307083\n",
      "the fbi\t\t-0.16669200076\t\t-0.261232779124\t\t-0.359247150178\t\t0.653842587232\n",
      "is strength\t\t-0.17641210768\t\t-0.224897681161\t\t-0.195173963602\t\t0.647991982331\n"
     ]
    }
   ],
   "source": [
    "def P4():\n",
    "### STUDENT START ###\n",
    "    cv = CountVectorizer()\n",
    "    v_train_data = cv.fit_transform(train_data)\n",
    "    logistic = LogisticRegression()\n",
    "    logistic.fit(v_train_data, train_labels)\n",
    "    features = cv.get_feature_names()\n",
    "    \n",
    "    # Create numpy array for each class for easier sorting\n",
    "    nparr0 = np.array(logistic.coef_[0])\n",
    "    nparr1 = np.array(logistic.coef_[1])\n",
    "    nparr2 = np.array(logistic.coef_[2])\n",
    "    nparr3 = np.array(logistic.coef_[3])\n",
    "    five0 = nparr0.argsort()[-5:][::-1]\n",
    "    five1 = nparr1.argsort()[-5:][::-1]\n",
    "    five2 = nparr2.argsort()[-5:][::-1]\n",
    "    five3 = nparr3.argsort()[-5:][::-1]\n",
    "    \n",
    "    rows = np.concatenate((five0, five1, five2, five3))\n",
    "    \n",
    "    print \"Feature\\t\\tClass1\\t\\tClass2\\t\\tClass3\\t\\tClass4\"\n",
    "    for row in rows:\n",
    "        print \"%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (features[row], logistic.coef_[0][row], logistic.coef_[1][row], logistic.coef_[2][row], logistic.coef_[3][row])\n",
    "      \n",
    "    print \"\\n\\n\"\n",
    "    \n",
    "    cv2 = CountVectorizer(ngram_range=(2,2))\n",
    "    v_train_data2 = cv2.fit_transform(train_data)\n",
    "    logistic2 = LogisticRegression()\n",
    "    logistic2.fit(v_train_data2, train_labels)\n",
    "    features2 = cv2.get_feature_names()\n",
    "    \n",
    "    # Create numpy array for each class for easier sorting\n",
    "    nparr0_2 = np.array(logistic2.coef_[0])\n",
    "    nparr1_2 = np.array(logistic2.coef_[1])\n",
    "    nparr2_2 = np.array(logistic2.coef_[2])\n",
    "    nparr3_2 = np.array(logistic2.coef_[3])\n",
    "    five0_2 = nparr0_2.argsort()[-5:][::-1]\n",
    "    five1_2 = nparr1_2.argsort()[-5:][::-1]\n",
    "    five2_2 = nparr2_2.argsort()[-5:][::-1]\n",
    "    five3_2 = nparr3_2.argsort()[-5:][::-1]\n",
    "    \n",
    "    rows2 = np.concatenate((five0_2, five1_2, five2_2, five3_2))\n",
    "    \n",
    "    print \"Feature\\t\\tClass1\\t\\tClass2\\t\\tClass3\\t\\tClass4\"\n",
    "    for row in rows2:\n",
    "        print \"%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (features2[row], logistic2.coef_[0][row], logistic2.coef_[1][row], logistic2.coef_[2][row], logistic2.coef_[3][row])\n",
    "    \n",
    "### STUDENT END ###\n",
    "P4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: It's actually a bit surprising that there's less \"stop words\" type of words in the bigrams. Of course some of them start with \"the\" or \"is\" and such but I would have expected more of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Try to improve the logistic regression classifier by passing a custom preprocessor to CountVectorizer. The preprocessing function runs on the raw text, before it is split into words by the tokenizer. Your preprocessor should try to normalize the input in various ways to improve generalization. For example, try lowercasing everything, replacing sequences of numbers with a single token, removing various other non-letter characters, and shortening long words. If you're not already familiar with regular expressions for manipulating strings, see https://docs.python.org/2/library/re.html, and re.sub() in particular. With your new preprocessor, how much did you reduce the size of the dictionary?\n",
    "\n",
    "For reference, I was able to improve dev F1 by 2 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty preprocessor score is: 0.702334008756\n",
      "Better preprocessor score is: 0.729936885015\n",
      "Length of vocabulary went from 33291 to 17498\n"
     ]
    }
   ],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "def better_preprocessor(s):\n",
    "### STUDENT START ###\n",
    "    stopwords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "    s = s.lower() # Make lowercase\n",
    "    s = re.sub('[0-9]+', '0', s) # Replace sequence of numbers by 0\n",
    "    s = re.sub('[\\W_]+', ' ', s) # Keep only alphanumeric characters\n",
    "    s = ' '.join([word for word in s.split() if word not in stopwords]) # Remove stopwords\n",
    "    s = ' '.join([word[:6] for word in s.split()]) # Limit to 6 characters (average English words is 5.1 chars)\n",
    "    \n",
    "    return s\n",
    "### STUDENT END ###\n",
    "\n",
    "def P5():\n",
    "### STUDENT START ###\n",
    "    cve = CountVectorizer(preprocessor=empty_preprocessor)\n",
    "    ve_train_data = cve.fit_transform(train_data)\n",
    "    ve_dev_data = cve.transform(dev_data)\n",
    "    loge = LogisticRegression()\n",
    "    loge.fit(ve_train_data, train_labels)\n",
    "    loge_pred = loge.predict(ve_dev_data)    \n",
    "    scoree = metrics.f1_score(dev_labels, loge_pred, average=\"weighted\")\n",
    "    print \"Empty preprocessor score is: %s\" % scoree\n",
    "    \n",
    "    cvb = CountVectorizer(preprocessor=better_preprocessor)\n",
    "    vb_train_data = cvb.fit_transform(train_data)\n",
    "    vb_dev_data = cvb.transform(dev_data)\n",
    "    logb = LogisticRegression()\n",
    "    logb.fit(vb_train_data, train_labels)\n",
    "    logb_pred = logb.predict(vb_dev_data)    \n",
    "    scoreb = metrics.f1_score(dev_labels, logb_pred, average=\"weighted\")\n",
    "    print \"Better preprocessor score is: %s\" % scoreb\n",
    "    \n",
    "    print \"Size of dictionary went from %s to %s\" % (ve_dev_data.shape[1], vb_dev_data.shape[1])\n",
    "### STUDENT END ###\n",
    "P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) The idea of regularization is to avoid learning very large weights (which are likely to fit the training data, but not generalize well) by adding a penalty to the total size of the learned weights. That is, logistic regression seeks the set of weights that minimizes errors in the training data AND has a small size. The default regularization, L2, computes this size as the sum of the squared weights (see P3, above). L1 regularization computes this size as the sum of the absolute values of the weights. The result is that whereas L2 regularization makes all the weights relatively small, L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "Train a logistic regression model using a \"l1\" penalty. Output the number of learned weights that are not equal to zero. How does this compare to the number of non-zero weights you get with \"l2\"? Now, reduce the size of the vocabulary by keeping only those features that have at least one non-zero weight and retrain a model using \"l2\".\n",
    "\n",
    "Make a plot showing accuracy of the re-trained model vs. the vocabulary size you get when pruning unused features by adjusting the C parameter.\n",
    "\n",
    "Note: The gradient descent code that trains the logistic regression model sometimes has trouble converging with extreme settings of the C parameter. Relax the convergence criteria by setting tol=.01 (the default is .0001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 12, 16, 205, 730, 1037, 1548, 2250, 2684]\n",
      "[0.33046718430123379, 0.43457103571275646, 0.46258739553955847, 0.7007827019398889, 0.67304618572546082, 0.68601005794842229, 0.68420139972094174, 0.68428805454906438, 0.70280150414731712]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzBCXpIb9P+GPbne1aThWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f58154117d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def P6():\n",
    "    # Keep this random seed here to make comparison easier.\n",
    "    np.random.seed(0)\n",
    "\n",
    "    ### STUDENT START ###\n",
    "    cv = CountVectorizer()\n",
    "    v_train_data = cv.fit_transform(train_data)\n",
    "    v_dev_data = cv.transform(dev_data)\n",
    "    \n",
    "    num_words = []\n",
    "    accuracy = []\n",
    "    \n",
    "    for cval in [0.001, 0.005, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]:\n",
    "        logistic = LogisticRegression(penalty='l1', C=cval, tol=0.01)\n",
    "        logistic.fit(v_train_data, train_labels)\n",
    "\n",
    "        #print logistic.coef_ # There appear to be much much more 0 weights\n",
    "\n",
    "        # Get array of the non zeros for each class\n",
    "        nparr0 = np.nonzero(logistic.coef_[0])\n",
    "        nparr1 = np.nonzero(logistic.coef_[1])\n",
    "        nparr2 = np.nonzero(logistic.coef_[2])\n",
    "        nparr3 = np.nonzero(logistic.coef_[3])\n",
    "\n",
    "        # Concatenate the arrays together and get the unique values\n",
    "        nonzeros = np.concatenate((nparr0[0], nparr1[0], nparr2[0], nparr3[0]))\n",
    "        nonzeros_unique = list(set(nonzeros))\n",
    "\n",
    "        # Add the number of features (words) to the num_words array for future plotting\n",
    "        num_words.append(len(nonzeros_unique))\n",
    "\n",
    "        v_train_data_pruned = v_train_data[:,nonzeros_unique]\n",
    "        v_dev_data_pruned  = v_dev_data[:,nonzeros_unique]\n",
    "\n",
    "        logistic2 = LogisticRegression(penalty='l2', tol=0.01)\n",
    "        logistic2.fit(v_train_data_pruned, train_labels)\n",
    "        log2_pred = logistic2.predict(v_dev_data_pruned)\n",
    "        score = metrics.f1_score(dev_labels, log2_pred, average=\"weighted\")\n",
    "        \n",
    "        # Add the score to the accuracy array for plotting\n",
    "        accuracy.append(score)\n",
    "        \n",
    "    print num_words\n",
    "    print accuracy\n",
    "    \n",
    "    plt.plot(num_words, accuracy, 'ro')\n",
    "    plt.axis([0, 3100, 0, 1])\n",
    "    plt.show()\n",
    "    ### STUDENT END ###\n",
    "P6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) Use the TfidfVectorizer -- how is this different from the CountVectorizer? Train a logistic regression model with C=100.\n",
    "\n",
    "Make predictions on the dev data and show the top 3 documents where the ratio R is largest, where R is:\n",
    "\n",
    "maximum predicted probability / predicted probability of the correct label\n",
    "\n",
    "What kinds of mistakes is the model making? Suggest a way to address one particular issue that you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label is: talk.religion.misc\n",
      "Actual label is: alt.atheism\n",
      "\n",
      "\n",
      "The 24 children were, of course, killed by a lone gunman in a second story\n",
      "window, who fired eight bullets in the space of two seconds...\n",
      "\n",
      "============================================\n",
      "Predicted label is: comp.graphics\n",
      "Actual label is: talk.religion.misc\n",
      "\n",
      "Can anyone provide me a ftp site where I can obtain a online version\n",
      "of the Book of Mormon. Please email the internet address if possible.\n",
      "============================================\n",
      "Predicted label is: comp.graphics\n",
      "Actual label is: talk.religion.misc\n",
      "\n",
      "I am pleased to announce that a *revised version* of _The Easy-to-Read Book\n",
      "of Mormon_ (former title: _Mormon's Book_) by Lynn Matthews Anderson is now\n",
      "available through anonymous ftp (see information below). In addition to the\n",
      "change in title, the revised ETR BOM has been shortened by several pages\n",
      "(eliminating many extraneous \"that's\" and \"of's\"), and many (minor) errors\n",
      "have been corrected. This release includes a simplified Joseph Smith Story,\n",
      "testimonies of the three and eight witnesses, and a \"Words-to-Know\"\n",
      "glossary.\n",
      "\n",
      "As with the previous announcement, readers are reminded that this is a\n",
      "not-for-profit endeavor. This is a copyrighted work, but people are welcome\n",
      "to make *verbatim* copies for personal use. People can recuperate the\n",
      "actual costs of printing (paper, copy center charges), but may not charge\n",
      "anything for their time in making copies, or in any way realize a profit\n",
      "from the use of this book. See the permissions notice in the book itself\n",
      "for the precise terms.\n",
      "\n",
      "Negotiations are currently underway with a Mormon publisher vis-a-vis the\n",
      "printing and distribution of bound books. (Sorry, I'm out of the wire-bound\n",
      "\"first editions.\") I will make another announcement about the availability\n",
      "of printed copies once everything has been worked out.\n",
      "\n",
      "FTP information: connect via anonymous ftp to carnot.itc.cmu.edu, then \"cd\n",
      "pub\" (you won't see anything at all until you do).\n",
      "\n",
      "\"The Easy-to-Read Book of Mormon\" is currently available in postscript and\n",
      "RTF (rich text format). (ASCII, LaTeX, and other versions can be made\n",
      "available; contact dba@andrew.cmu.edu for details.) You should be able to\n",
      "print the postscript file on any postscript printer (such as an Apple\n",
      "Laserwriter); let dba know if you have any difficulties. (The postscript in\n",
      "the last release had problems on some printers; this time it should work\n",
      "better.) RTF is a standard document interchange format that can be read in\n",
      "by a number of word processors, including Microsoft Word for both the\n",
      "Macintosh and Windows. If you don't have a postscript printer, you may be\n",
      "able to use the RTF file to print out a copy of the book.\n",
      "\n",
      "-r--r--r--  1 dba                   1984742 Apr 27 13:12 etrbom.ps\n",
      "-r--r--r--  1 dba                   1209071 Apr 27 13:13 etrbom.rtf\n",
      "\n",
      "For more information about how this project came about, please refer to my\n",
      "article in the current issue of _Sunstone_, entitled \"Delighting in\n",
      "Plainness: Issues Surrounding a Simple Modern English Book of Mormon.\"\n",
      "\n",
      "Send all inquiries and comments to:\n",
      "\n",
      "    Lynn Matthews Anderson\n",
      "    5806 Hampton Street\n",
      "    Pittsburgh, PA 15206\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "def P7():\n",
    "### STUDENT START ###\n",
    "    tv = TfidfVectorizer()\n",
    "    v_train_data = tv.fit_transform(train_data)\n",
    "    v_dev_data = tv.transform(dev_data)\n",
    "    \n",
    "    logistic = LogisticRegression(C=100)\n",
    "    logistic.fit(v_train_data, train_labels)\n",
    "    \n",
    "    R_values = []\n",
    "    for i in range(len(dev_data)):\n",
    "        probs = logistic.predict_proba(v_dev_data[i])\n",
    "        R = max(probs[0])/probs[0][dev_labels[i]]\n",
    "        R_values.append(R)\n",
    "    \n",
    "    npR = np.array(R_values)\n",
    "    indexes = npR.argsort()[-3:]\n",
    "    docs = [dev_data[x] for x in indexes]\n",
    "    \n",
    "    for i in range(len(docs)):\n",
    "        print \"Predicted label is: %s\" % newsgroups_train.target_names[logistic.predict(v_dev_data[indexes[i]])[0]]\n",
    "        print \"Actual label is: %s\\n\" % newsgroups_train.target_names[dev_labels[indexes[i]]]\n",
    "        print docs[i]\n",
    "        print \"============================================\"\n",
    "## STUDENT END ###\n",
    "P7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: TfidfVectorizer is different than CountVectorizer in that it normalizes its results.\n",
    "\n",
    "The three main errors above all have to do with the religion category in one way or another. In the second and third examples, while the fact that they contain \"Book of Mormon\" should make it clear that they are religion based texts, the fact that there are also many technical words contained in the text is likely throwing off the prediction. One possible way around this would be to include 2grams and 3grams. However, this may make our feature set too large. Another possibility would be to prune certain common words from the features. These would have to be decided based on the context however. Given that these texts come from online posts, perhaps words such as \"online\" and \"internet\" could be eliminated in this context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
